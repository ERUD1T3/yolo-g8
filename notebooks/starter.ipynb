{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning MTH 5320 \n",
    "## Homework 3\n",
    "## 12/1/2023\n",
    "## By Josias Moukpe"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ba81126aaed913f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Object Detection Model Assignment\n",
    "\n",
    "### Dataset Prep and Preprocessing (20 points)\n",
    "- Collect or create a dataset of images containing the objects you want to detect annotated with bounding boxes.\n",
    "- Implement data preprocessing techniques, including:\n",
    "  - Resizing images\n",
    "  - Normalizing pixels\n",
    "- Hint: You can find pre-annotated datasets for download on [Roboflow](https://github.com/roboflow/supervision).\n",
    "\n",
    "### Model Architecture (30 points)\n",
    "- Train a YOLOv8 model on your selected dataset using the public repository available at [Ultralytics](https://github.com/ultralytics/ultralytics).\n",
    "- Monitor the training progress and use appropriate techniques to prevent overfitting.\n",
    "- Conduct at least 3 tuning experiments.\n",
    "\n",
    "### Evaluation (20 points)\n",
    "- Evaluate your trained model's performance on a test dataset using mAP (mean Average Precision).\n",
    "- Visualize the model's predictions on sample images.\n",
    "- Analyze the model's performance, discussing any specific failure modes on your test dataset.\n",
    "\n",
    "### BONUS (25 bonus points)\n",
    "- Design and train a custom CNN-based object detection model using a self-designed architecture.\n",
    "- Alternatively, compare several publicly available models on the same dataset.\n",
    "- Consider implementing Gaussian YOLO V8 and comparing it against YOLO V8.\n",
    "\n",
    "Please ensure that your code includes docstrings and type hints as requested, and be thorough in your analysis and experimentation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5b0040c7ba5685e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup and Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff2fced5a3fdcef8"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-06T05:11:40.513815500Z",
     "start_time": "2023-12-06T05:11:37.031657200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using {device} device')\n",
    "# get device specs\n",
    "print(torch.cuda.get_device_name(0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T05:12:30.554289400Z",
     "start_time": "2023-12-06T05:12:30.442303100Z"
    }
   },
   "id": "ebbae101c1442e47"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# test of yolo  v8\n",
    "model = YOLO('yolov8n.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T05:12:32.898585Z",
     "start_time": "2023-12-06T05:12:32.762094Z"
    }
   },
   "id": "2244d1d3cd0b39af"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# vid_path = \"../data/test_footage.mp4\"\n",
    "# results = model(source=vid_path, show=True, conf=0.4, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T05:12:33.904520Z",
     "start_time": "2023-12-06T05:12:33.860536100Z"
    }
   },
   "id": "423ea9aa1b17b39"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "debc7d0f0e631a0"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "ocr_dataset_path = 'D:/College/Fall2023/DeepLearning/yolo_ocr'\n",
    "yaml_path = 'D:/College/Fall2023/DeepLearning/yolo_ocr/data.yaml'\n",
    "# labels already in yolo v8 format"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T05:12:36.297729600Z",
     "start_time": "2023-12-06T05:12:36.245740900Z"
    }
   },
   "id": "3e86c5b75e21151"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.0.222 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.221 🚀 Python-3.9.18 torch-2.1.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3060, 12287MiB)\n",
      "\u001B[34m\u001B[1mengine\\trainer: \u001B[0mtask=detect, mode=train, model=yolov8n.pt, data=D:/College/Fall2023/DeepLearning/yolo_ocr/data.yaml, epochs=10, patience=50, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=yolov8n_ocr2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\yolov8n_ocr2\n",
      "Overriding model.yaml nc=80 with nc=36\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    758332  ultralytics.nn.modules.head.Detect           [36, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3017868 parameters, 3017852 gradients, 8.2 GFLOPs\n",
      "Transferred 319/355 items from pretrained weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33merud1t3\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.0"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\the_3\\OneDrive\\Desktop\\school\\Fall 2023\\yolo-g8\\notebooks\\wandb\\run-20231206_001258-z8uow0e5</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/erud1t3/YOLOv8/runs/z8uow0e5' target=\"_blank\">yolov8n_ocr2</a></strong> to <a href='https://wandb.ai/erud1t3/YOLOv8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/erud1t3/YOLOv8' target=\"_blank\">https://wandb.ai/erud1t3/YOLOv8</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/erud1t3/YOLOv8/runs/z8uow0e5' target=\"_blank\">https://wandb.ai/erud1t3/YOLOv8/runs/z8uow0e5</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001B[34m\u001B[1mAMP: \u001B[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001B[34m\u001B[1mAMP: \u001B[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mtrain: \u001B[0mScanning D:\\College\\Fall2023\\DeepLearning\\yolo_ocr\\train\\labels.cache... 4245 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4245/4245 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mtrain: \u001B[0mWARNING ⚠️ D:\\College\\Fall2023\\DeepLearning\\yolo_ocr\\train\\images\\03c3dfc7e998b8562_jpg.rf.3d9d76c2437bcfb5cf433c0b6306d1ac.jpg: 1 duplicate labels removed\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mWARNING ⚠️ D:\\College\\Fall2023\\DeepLearning\\yolo_ocr\\train\\images\\185d5dfa193c4ced_jpg.rf.b3d2ab34086b9e8f050b514cc22b4b9a.jpg: 1 duplicate labels removed\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mWARNING ⚠️ D:\\College\\Fall2023\\DeepLearning\\yolo_ocr\\train\\images\\24c1fc0bf63da8153_jpg.rf.bd40f6e750b1a7990a9945aadddc1ef1.jpg: 1 duplicate labels removed\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mWARNING ⚠️ D:\\College\\Fall2023\\DeepLearning\\yolo_ocr\\train\\images\\421ca0ce7f9329622_jpg.rf.4e25470bde67152fea6834e88e803708.jpg: 1 duplicate labels removed\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mWARNING ⚠️ D:\\College\\Fall2023\\DeepLearning\\yolo_ocr\\train\\images\\4572990fd64bb6be3_jpg.rf.7ce23d5cd8b919a7832485c4815d97c8.jpg: 1 duplicate labels removed\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mWARNING ⚠️ D:\\College\\Fall2023\\DeepLearning\\yolo_ocr\\train\\images\\492f6d607aae58c12_jpg.rf.44d8f6885cd8664eeff661809ad807ab.jpg: 1 duplicate labels removed\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mWARNING ⚠️ D:\\College\\Fall2023\\DeepLearning\\yolo_ocr\\train\\images\\624860d2c1d529cf3_jpg.rf.0e1b82945bb5fb1b4402a8a8039f45b4.jpg: 1 duplicate labels removed\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mWARNING ⚠️ D:\\College\\Fall2023\\DeepLearning\\yolo_ocr\\train\\images\\6520279c853247d5_jpg.rf.79ecc5c23c55675d0e25a1b5e0b07a9a.jpg: 1 duplicate labels removed\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mWARNING ⚠️ D:\\College\\Fall2023\\DeepLearning\\yolo_ocr\\train\\images\\66144e8ccb6114bc_jpg.rf.a5bd76659353fe5df4d9ca50d0d2a727.jpg: 1 duplicate labels removed\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mWARNING ⚠️ D:\\College\\Fall2023\\DeepLearning\\yolo_ocr\\train\\images\\73bba7d7f439b76f_jpg.rf.59cfcd935931741505777badc23b52ae.jpg: 1 duplicate labels removed\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mWARNING ⚠️ D:\\College\\Fall2023\\DeepLearning\\yolo_ocr\\train\\images\\86063e808f33e6a5_jpg.rf.fed978bb4e241d36376321e03a32df55.jpg: 1 duplicate labels removed\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mWARNING ⚠️ D:\\College\\Fall2023\\DeepLearning\\yolo_ocr\\train\\images\\d3de1c42196d3b3e2_jpg.rf.c9b4b5a50d4bc11c338c33ddf29f56c1.jpg: 1 duplicate labels removed\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mWARNING ⚠️ D:\\College\\Fall2023\\DeepLearning\\yolo_ocr\\train\\images\\e8a1a9c789e6ee6e_jpg.rf.c434c9ae469eb57e14fdd5cee9c8a135.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning D:\\College\\Fall2023\\DeepLearning\\yolo_ocr\\valid\\labels.cache... 1221 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1221/1221 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mval: \u001B[0mWARNING ⚠️ D:\\College\\Fall2023\\DeepLearning\\yolo_ocr\\valid\\images\\0ecee0dd32848030_jpg.rf.ffe2fb185ba29646fcf16eacf0e99021.jpg: 1 duplicate labels removed\n",
      "\u001B[34m\u001B[1mval: \u001B[0mWARNING ⚠️ D:\\College\\Fall2023\\DeepLearning\\yolo_ocr\\valid\\images\\22deb87614a920e6_jpg.rf.a72d572c1920415b880c030e7b1a17e6.jpg: 1 duplicate labels removed\n",
      "\u001B[34m\u001B[1mval: \u001B[0mWARNING ⚠️ D:\\College\\Fall2023\\DeepLearning\\yolo_ocr\\valid\\images\\dcb210b4cae8edf22_jpg.rf.286248320e4fe0f26c75339865578ccc.jpg: 1 duplicate labels removed\n",
      "\u001B[34m\u001B[1mval: \u001B[0mWARNING ⚠️ D:\\College\\Fall2023\\DeepLearning\\yolo_ocr\\valid\\images\\f7f068d4fb49a4f34_jpg.rf.8856a3518481e9dcc022b1a802a966cd.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\yolov8n_ocr2\\labels.jpg... \n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m AdamW(lr=0.00025, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001B[1mruns\\detect\\yolov8n_ocr2\u001B[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10      1.55G      1.167      3.683      1.242         21        640: 100%|██████████| 531/531 [01:05<00:00,  8.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 77/77 [00:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1221       7890      0.242      0.344      0.195      0.144\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10      1.56G      0.962      2.144      1.147         31        640: 100%|██████████| 531/531 [01:02<00:00,  8.51it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 77/77 [00:14<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1221       7890       0.63       0.68      0.691      0.525\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10      1.57G     0.8936      1.397      1.117         34        640: 100%|██████████| 531/531 [01:00<00:00,  8.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 77/77 [00:16<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1221       7890      0.835       0.79      0.846      0.638\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10      1.56G     0.8491      1.084       1.09         29        640: 100%|██████████| 531/531 [00:56<00:00,  9.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 77/77 [00:12<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1221       7890      0.888      0.827      0.889      0.683\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10      1.55G     0.8163     0.9371      1.065         35        640: 100%|██████████| 531/531 [00:56<00:00,  9.44it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 77/77 [00:12<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1221       7890      0.914      0.856      0.908        0.7\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10      1.56G     0.7832     0.8403      1.046         28        640: 100%|██████████| 531/531 [00:56<00:00,  9.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 77/77 [00:12<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1221       7890       0.93      0.856      0.917      0.713\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10      1.56G     0.7665     0.7881      1.036         31        640: 100%|██████████| 531/531 [00:56<00:00,  9.47it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 77/77 [00:11<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1221       7890      0.943      0.865      0.926      0.727\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10      1.56G     0.7412     0.7372      1.023         28        640: 100%|██████████| 531/531 [00:56<00:00,  9.47it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 77/77 [00:11<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1221       7890      0.929       0.88       0.93      0.724\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10      1.56G     0.7291     0.7062      1.011         23        640: 100%|██████████| 531/531 [00:56<00:00,  9.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 77/77 [00:11<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1221       7890      0.934      0.877      0.931      0.733\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10      1.56G     0.7105     0.6777      1.005         32        640: 100%|██████████| 531/531 [00:55<00:00,  9.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 77/77 [00:11<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1221       7890      0.938       0.88      0.934      0.736\n",
      "\n",
      "10 epochs completed in 0.211 hours.\n",
      "Optimizer stripped from runs\\detect\\yolov8n_ocr2\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\yolov8n_ocr2\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\yolov8n_ocr2\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.221 🚀 Python-3.9.18 torch-2.1.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3060, 12287MiB)\n",
      "Model summary (fused): 168 layers, 3012668 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 77/77 [00:14<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1221       7890      0.936      0.881      0.934      0.736\n",
      "                     0       1221        408      0.748      0.917      0.891      0.713\n",
      "                     1       1221        386      0.846       0.91       0.93      0.654\n",
      "                     2       1221        399      0.964      0.955      0.981      0.773\n",
      "                     3       1221        369      0.966      0.962       0.99      0.787\n",
      "                     4       1221        326       0.98      0.901      0.977       0.75\n",
      "                     5       1221        383      0.959      0.963      0.988      0.791\n",
      "                     6       1221        393      0.977      0.968      0.981      0.783\n",
      "                     7       1221        343      0.963      0.965       0.99      0.746\n",
      "                     8       1221        346      0.913      0.948      0.973      0.762\n",
      "                     9       1221        324      0.984      0.976      0.992      0.794\n",
      "                     A       1221        235       0.95      0.973      0.982      0.781\n",
      "                     B       1221        210      0.948      0.861      0.962      0.772\n",
      "                     C       1221        161      0.919      0.911      0.967      0.755\n",
      "                     D       1221        173      0.905      0.798      0.925      0.751\n",
      "                     E       1221        176      0.978      0.949      0.991      0.812\n",
      "                     F       1221        158      0.967      0.962      0.979      0.768\n",
      "                     G       1221        184      0.965      0.909      0.984      0.806\n",
      "                     H       1221        197      0.995      0.941      0.985      0.773\n",
      "                     I       1221        147      0.897      0.551      0.841      0.622\n",
      "                     J       1221        153      0.956      0.895      0.961      0.742\n",
      "                     K       1221        131      0.932      0.924      0.967      0.764\n",
      "                     L       1221        181       0.94      0.901      0.957      0.737\n",
      "                     M       1221        183       0.92      0.809      0.926      0.731\n",
      "                     N       1221        160      0.788      0.919      0.927      0.736\n",
      "                     O       1221        130      0.618      0.577      0.679      0.565\n",
      "                     P       1221        169      0.978      0.953      0.985       0.79\n",
      "                     Q       1221         48          1          0      0.102     0.0845\n",
      "                     R       1221        173      0.982      0.966      0.986      0.799\n",
      "                     S       1221        195      0.979      0.897      0.968      0.772\n",
      "                     T       1221        171      0.942       0.93      0.958      0.749\n",
      "                     U       1221        161      0.963      0.958      0.973      0.771\n",
      "                     V       1221        167      0.981      0.924      0.981      0.751\n",
      "                     W       1221        152      0.937      0.914      0.965      0.757\n",
      "                     X       1221        158       0.99      0.962      0.992      0.785\n",
      "                     Y       1221        122      0.991      0.944      0.982      0.751\n",
      "                     Z       1221        118      0.991      0.934      0.989       0.81\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.0ms loss, 1.6ms postprocess per image\n",
      "Results saved to \u001B[1mruns\\detect\\yolov8n_ocr2\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='10.545 MB of 10.545 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04839f3d1e59449f96b28aa4b6212612"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>▂▆█▇▇▆▅▃▂▁</td></tr><tr><td>lr/pg1</td><td>▂▆█▇▇▆▅▃▂▁</td></tr><tr><td>lr/pg2</td><td>▂▆█▇▇▆▅▃▂▁</td></tr><tr><td>metrics/mAP50(B)</td><td>▁▆▇███████</td></tr><tr><td>metrics/mAP50-95(B)</td><td>▁▆▇▇██████</td></tr><tr><td>metrics/precision(B)</td><td>▁▅▇▇██████</td></tr><tr><td>metrics/recall(B)</td><td>▁▅▇▇██████</td></tr><tr><td>model/GFLOPs</td><td>▁</td></tr><tr><td>model/parameters</td><td>▁</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>▁</td></tr><tr><td>train/box_loss</td><td>█▅▄▃▃▂▂▁▁▁</td></tr><tr><td>train/cls_loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>train/dfl_loss</td><td>█▅▄▄▃▂▂▂▁▁</td></tr><tr><td>val/box_loss</td><td>█▅▄▄▃▂▁▂▁▁</td></tr><tr><td>val/cls_loss</td><td>█▄▂▂▂▁▁▁▁▁</td></tr><tr><td>val/dfl_loss</td><td>█▆▅▄▃▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>5e-05</td></tr><tr><td>lr/pg1</td><td>5e-05</td></tr><tr><td>lr/pg2</td><td>5e-05</td></tr><tr><td>metrics/mAP50(B)</td><td>0.93354</td></tr><tr><td>metrics/mAP50-95(B)</td><td>0.73574</td></tr><tr><td>metrics/precision(B)</td><td>0.93642</td></tr><tr><td>metrics/recall(B)</td><td>0.88123</td></tr><tr><td>model/GFLOPs</td><td>8.232</td></tr><tr><td>model/parameters</td><td>3017868</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>4.077</td></tr><tr><td>train/box_loss</td><td>0.71051</td></tr><tr><td>train/cls_loss</td><td>0.67766</td></tr><tr><td>train/dfl_loss</td><td>1.00462</td></tr><tr><td>val/box_loss</td><td>0.81648</td></tr><tr><td>val/cls_loss</td><td>0.59504</td></tr><tr><td>val/dfl_loss</td><td>0.97734</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">yolov8n_ocr2</strong> at: <a href='https://wandb.ai/erud1t3/YOLOv8/runs/z8uow0e5' target=\"_blank\">https://wandb.ai/erud1t3/YOLOv8/runs/z8uow0e5</a><br/> View job at <a href='https://wandb.ai/erud1t3/YOLOv8/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyMTIzMzM0Nw==/version_details/v0' target=\"_blank\">https://wandb.ai/erud1t3/YOLOv8/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyMTIzMzM0Nw==/version_details/v0</a><br/>Synced 6 W&B file(s), 21 media file(s), 7 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20231206_001258-z8uow0e5\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training.\n",
    "results = model.train(\n",
    "    data=yaml_path,\n",
    "    imgsz=640,\n",
    "    epochs=1000,\n",
    "    batch=8,\n",
    "    name='yolov8n_ocr')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T05:27:38.988087300Z",
     "start_time": "2023-12-06T05:12:51.729769700Z"
    }
   },
   "id": "d1ce912d2edf32eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85b3555bc208874"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "59dccdb1e2ca7d4d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Custom Yolo G8 Architecture in Pytorch (NO UltraLytics here unfortunately)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "514485cf9c563ab1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
